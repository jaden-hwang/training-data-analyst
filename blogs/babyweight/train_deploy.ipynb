{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Structured data prediction using Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using Dataflow\n",
    "<li> Creating a model using the high-level Estimator API \n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying model\n",
    "<li> Predicting with model\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://cloud-training-demos-ml/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi\n",
    "gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the ever_born (the total number of babies born to this mother), the factors all seem to play a part in the baby's weight.  Male babies are heavier on average than female babies. The mother's age and race play a part (age much more than race -- teenage and middle-aged moms tend to have lower-weight babies).  Twins, triplets, etc. are lower weight than single births.  Preemies weigh in lower as do babies born to single moms. Moms who use alcohol or cigarettes have babies that weigh lower on average.\n",
    "<p>\n",
    "In the rest of this notebook, we will use machine learning to combine all of these factors to come up with a prediction of a baby's weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create ML dataset using Dataflow </h2>\n",
    "<p>\n",
    "We can use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. \n",
    "\n",
    "Instead of using Beam/Dataflow, I had two other options:\n",
    "<ol>\n",
    "<li> Read from BigQuery directly using TensorFlow. However, using CSV files gives us the advantage of shuffling during read. This is important for distributed training because some workers might be slower than others, and shuffling the data helps prevent the same data from being assigned to the slow workers.\n",
    "<li> Use the BigQuery console (http://bigquery.cloud.google.com) to run a Query and save the result as a CSV file. For larger datasets, you may have to select the option to \"allow large results\" and save the result into a CSV file on Google Cloud Storage. However, in this case, I want to do some preprocessing (on the \"race\" column). If I didn't need preprocessing, I could have used the web console. Also, I prefer to script it out rather than run queries on the user interface, so I am using Cloud Dataflow for the preprocessing.\n",
    "</ol>\n",
    "\n",
    "<p>\n",
    "Note that I have set in_test_mode=True in the following code -- this will run the code locally on a small subset of the data -- the full results were copied into your bucket using the following code:\n",
    "<pre>\n",
    "gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}\n",
    "</pre>\n",
    "If you are running in your own project, change in_test_mode=False; after you launch this, the notebook will appear to be hung. Go to the GCP webconsole to the Dataflow section and monitor the running job. It took about <b>30 minutes</b> for me with autoscaling to 15 workers -- Qwiklabs accounts won't be able scale to that level, and so doing it on Qwiklabs would have taken 3-4 hours. Hence, the short-cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-babyweight-features-170412-063852 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/apache_beam/coders/typecoders.py:136: UserWarning: Using fallback coder for typehint: Any.\n",
      "  warnings.warn('Using fallback coder for typehint: %r.' % typehint)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "\n",
    "def to_csv(rowdict):\n",
    "    # pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use'.split(',')\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = copy.deepcopy(rowdict)\n",
    "    if 'mother_race' in rowdict and rowdict['mother_race'] in races:\n",
    "      result['mother_race'] = races[rowdict['mother_race']]\n",
    "    else:\n",
    "      result['mother_race'] = 'Unknown'\n",
    "    \n",
    "    data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "    key = hashlib.sha224(data).hexdigest()  # hash the columns to form a key\n",
    "    return str('{},{}'.format(data, key))\n",
    "  \n",
    "def preprocess(in_test_mode):\n",
    "  job_name = 'preprocess-babyweight-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    \n",
    "  if in_test_mode:\n",
    "    print 'Launching local job ... hang on'\n",
    "    OUTPUT_DIR = './preproc_df'\n",
    "  else:\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/babyweight/preproc_df/'.format(BUCKET)\n",
    "    \n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  if in_test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "  else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "  p = beam.Pipeline(RUNNER, options=opts)\n",
    "  query = \"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  mother_race,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  mother_married,\n",
    "  ever_born,\n",
    "  cigarette_use,\n",
    "  alcohol_use,\n",
    "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "    \"\"\"\n",
    "  \n",
    "  if in_test_mode:\n",
    "    query = query + ' LIMIT 100' \n",
    "  \n",
    "  for step in ['train', 'eval']:\n",
    "    if step == 'train':\n",
    "      selquery = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) < 3'.format(query)\n",
    "    else:\n",
    "      selquery = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) = 3'.format(query)\n",
    "\n",
    "    (p \n",
    "     | '{}_read'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True))\n",
    "     | '{}_csv'.format(step) >> beam.Map(to_csv)\n",
    "     | '{}_out'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, '{}.csv'.format(step))))\n",
    "    )\n",
    " \n",
    "  job = p.run()\n",
    "  \n",
    "preprocess(in_test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dataflow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/babyweight/preproc/eval.csv-00000-of-00016\n",
      "gs://cloud-training-demos-ml/babyweight/preproc/train.csv-00000-of-00040\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create TensorFlow model using TensorFlow's Estimator API </h2>\n",
    "<p>\n",
    "First, write an input_fn to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]\n",
    "TRAIN_STEPS = 1000\n",
    "\n",
    "def read_dataset(prefix, pattern, batch_size=512):\n",
    "  # use prefix to create filename\n",
    "  filename = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, pattern)\n",
    "  if prefix == 'train':\n",
    "    mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "  else:\n",
    "    mode = tf.contrib.learn.ModeKeys.EVAL\n",
    "    \n",
    "  # the actual input function passed to TensorFlow\n",
    "  def _input_fn():\n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, shuffle=True)\n",
    " \n",
    "    # read CSV\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    features.pop(KEY_COLUMN)\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "  # define column types\n",
    "  races = ['White', 'Black', 'American Indian', 'Chinese', \n",
    "           'Japanese', 'Hawaiian', 'Filipino', 'Unknown',\n",
    "           'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']\n",
    "  is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use = \\\n",
    "   [ \\\n",
    "    tflayers.sparse_column_with_keys('is_male', keys=['True', 'False']),\n",
    "    tflayers.real_valued_column('mother_age'),\n",
    "    tflayers.sparse_column_with_keys('mother_race', keys=races),\n",
    "    tflayers.real_valued_column('plurality'),\n",
    "    tflayers.real_valued_column('gestation_weeks'),\n",
    "    tflayers.sparse_column_with_keys('mother_married', keys=['True', 'False']),\n",
    "    tflayers.sparse_column_with_keys('cigarette_use', keys=['True', 'False', 'None']),\n",
    "    tflayers.sparse_column_with_keys('alcohol_use', keys=['True', 'False', 'None'])\n",
    "    ]\n",
    "\n",
    "  # which columns are wide (sparse, linear relationship to output) and which are deep (complex relationship to output?)  \n",
    "  wide = [is_male, mother_race, plurality, mother_married, cigarette_use, alcohol_use]\n",
    "  deep = [\\\n",
    "                mother_age,\n",
    "                gestation_weeks,\n",
    "                tflayers.embedding_column(mother_race, 3)\n",
    "               ]\n",
    "  return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the TensorFlow model, we also need a serving input function. We will want all the inputs from our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      'is_male': tf.placeholder(tf.string, [None]),\n",
    "      'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "      'mother_race': tf.placeholder(tf.string, [None]),\n",
    "      'plurality': tf.placeholder(tf.float32, [None]),\n",
    "      'gestation_weeks': tf.placeholder(tf.float32, [None]),\n",
    "      'mother_married': tf.placeholder(tf.string, [None]),\n",
    "      'cigarette_use': tf.placeholder(tf.string, [None]),\n",
    "      'alcohol_use': tf.placeholder(tf.string, [None])\n",
    "    }\n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tflearn.utils.input_fn_utils.InputFnOps(\n",
    "      features,\n",
    "      None,\n",
    "      feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "\n",
    "pattern = \"00001-of-\"  # process only one of the shards, for testing purposes\n",
    "\n",
    "def experiment_fn(output_dir):\n",
    "    wide, deep = get_wide_deep()\n",
    "    return tflearn.Experiment(\n",
    "        tflearn.DNNLinearCombinedRegressor(model_dir=output_dir,\n",
    "                                           linear_feature_columns=wide,\n",
    "                                           dnn_feature_columns=deep,\n",
    "                                           dnn_hidden_units=[64, 32]),\n",
    "        train_input_fn=read_dataset('train', pattern),\n",
    "        eval_input_fn=read_dataset('eval', pattern),\n",
    "        eval_metrics={\n",
    "            'rmse': tflearn.MetricSpec(\n",
    "                metric_fn=metrics.streaming_root_mean_squared_error\n",
    "            )\n",
    "        },\n",
    "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "            serving_input_fn,\n",
    "            default_output_alternative_key=None,\n",
    "            exports_to_keep=1\n",
    "        )],\n",
    "        train_steps=TRAIN_STEPS\n",
    "    )\n",
    "\n",
    "shutil.rmtree('babyweight_trained', ignore_errors=True) # start fresh each time\n",
    "learn_runner.run(experiment_fn, 'babyweight_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TensorFlow code working on a subset of the data (in the code above, I was reading only the 00001-of-x file), we can package the TensorFlow code up as a Python module and train it on Cloud ML Engine.\n",
    "<p>\n",
    "<h2> Train on Cloud ML Engine </h2>\n",
    "<p>\n",
    "Training on Cloud ML Engine requires:\n",
    "<ol>\n",
    "<li> Making the code a Python package\n",
    "<li> Using gcloud to submit the training code to Cloud ML Engine\n",
    "</ol>\n",
    "<p>\n",
    "The code in model.py is the same as in the above cells. I just moved it to a file so that I could package it up as a module.\n",
    "(explore the <a href=\"babyweight/trainer\">directory structure</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_dataset(prefix, batch_size=512):\n",
      "def get_wide_deep():\n",
      "def serving_input_fn():\n",
      "def experiment_fn(output_dir):\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "grep \"^def\" babyweight/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After moving the code to a package, make sure it works standalone. (Note the --pattern and --train_steps lines so that I am not trying to boil the ocean on my laptop). Even then, this takes about <b>a minute</b> in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf babyweight_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python -m trainer.task \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=babyweight_trained \\\n",
    "   --job-dir=./tmp \\\n",
    "  --pattern=\"00001-of-\" --train_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode, you can run it on Cloud ML Engine.  Note that doing it on the entire dataset will take a while. The training run took about <b> an hour </b> for me on STANDARD_1. So, for the purposes of Qwiklabs, this is done on a partial dataset on a smaller tier (BASIC_1).  You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/babyweight/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --pattern=\"00001-of-\" \\\n",
    "   --train_steps=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training finished with a RMSE of 1 lb.  Obviously, this is our first model. We could probably add in some features, discretize the mother's age, and do some hyper-parameter tuning to get to a lower RMSE.  I'll leave that to you.  If you create a better model, I'd love to hear about it -- please do write a short blog post about what you did, and tweet it at me -- @lak_gcp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 28187. Click <a href=\"/_proxy/51275/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "28187"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/babyweight/trained_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 28187\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=\"70%\">\n",
    "<tr><td><img src=\"weights.png\"/></td><td><img src=\"rmse.png\" /></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Deploy trained model </h2>\n",
    "<p>\n",
    "Deploying the trained model to act as a REST web service is a simple gcloud call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/babyweight/trained_model/export/Servo/\n",
      "gs://cloud-training-demos-ml/babyweight/trained_model/export/Servo/1492051542987/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model/export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying babyweight v1 from gs://cloud-training-demos-ml/babyweight/trained_model/export/Servo/1492051542987/ ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "...........................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/export/Servo/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Use model to predict </h2>\n",
    "<p>\n",
    "Send a JSON request to the endpoint of the service to make it predict a baby's weight ... I am going to try out how well the model would have predicted the weights of our two kids and a couple of variations while we are at it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response={u'predictions': [{u'outputs': 7.30388879776001}, {u'outputs': 6.767232418060303}, {u'outputs': 7.850516319274902}, {u'outputs': 6.196153163909912}]}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'False',\n",
    "        'mother_age': 29.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 38,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 2.0,\n",
    "        'gestation_weeks': 37,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'babyweight', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model, our son would have clocked in at 7.3 lbs and our daughter at 6.8 lbs.\n",
    "<p>\n",
    "The weights are off by about 0.5 lbs. Pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
