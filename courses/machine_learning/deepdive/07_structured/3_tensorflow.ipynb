{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating TensorFlow model </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating a model using the high-level Estimator API \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'asl-ml-immersion-temp'\n",
    "PROJECT = 'asl-ml-immersion'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.csv\n",
      "train.csv\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create TensorFlow model using TensorFlow's Estimator API </h2>\n",
    "<p>\n",
    "First, write an input_fn to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]\n",
    "TRAIN_STEPS = 1000\n",
    "\n",
    "def read_dataset(prefix, pattern, batch_size=512):\n",
    "  # use prefix to create filename\n",
    "  filename = './{}*{}*'.format(prefix, pattern)\n",
    "  if prefix == 'train':\n",
    "    mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "  else:\n",
    "    mode = tf.contrib.learn.ModeKeys.EVAL\n",
    "    \n",
    "  # the actual input function passed to TensorFlow\n",
    "  def _input_fn():\n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, shuffle=True)\n",
    " \n",
    "    # read CSV\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    features.pop(KEY_COLUMN)\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "  # define column types\n",
    "  races = ['White', 'Black', 'American Indian', 'Chinese', \n",
    "           'Japanese', 'Hawaiian', 'Filipino', 'Unknown',\n",
    "           'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']\n",
    "  is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use = \\\n",
    "   [ \\\n",
    "    tflayers.sparse_column_with_keys('is_male', keys=['True', 'False']),\n",
    "    tflayers.real_valued_column('mother_age'),\n",
    "    tflayers.sparse_column_with_keys('mother_race', keys=races),\n",
    "    tflayers.real_valued_column('plurality'),\n",
    "    tflayers.real_valued_column('gestation_weeks'),\n",
    "    tflayers.sparse_column_with_keys('mother_married', keys=['True', 'False']),\n",
    "    tflayers.sparse_column_with_keys('cigarette_use', keys=['True', 'False', 'None']),\n",
    "    tflayers.sparse_column_with_keys('alcohol_use', keys=['True', 'False', 'None'])\n",
    "    ]\n",
    "\n",
    "  # transformations\n",
    "  plurality_b = tflayers.bucketized_column(plurality, boundaries=np.arange(0.5, 5.5, 1.0).tolist())\n",
    "  mother_age_b = tflayers.bucketized_column(mother_age, boundaries=np.arange(10, 40, 5).tolist())\n",
    "  gestation_b = tflayers.bucketized_column(gestation_weeks, boundaries=[25, 30, 35, 38, 40])\n",
    "  mother_race_e = tflayers.embedding_column(mother_race, 3)\n",
    "  crosses = tflayers.crossed_column([mother_age_b, plurality_b, gestation_b], hash_bucket_size=10)\n",
    "\n",
    "  # which columns are wide (sparse, linear relationship to output) and which are deep (complex relationship to output?)\n",
    "  wide = [is_male, mother_race, \n",
    "          plurality_b, mother_age_b, gestation_b, crosses,\n",
    "          mother_married, cigarette_use, alcohol_use]\n",
    "  deep = [\\\n",
    "                mother_age,\n",
    "                plurality,\n",
    "                gestation_weeks,\n",
    "                mother_race_e\n",
    "               ]\n",
    "  return wide, deep\n",
    "\n",
    "  return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the TensorFlow model, we also need a serving input function. We will want all the inputs from our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      'is_male': tf.placeholder(tf.string, [None]),\n",
    "      'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "      'mother_race': tf.placeholder(tf.string, [None]),\n",
    "      'plurality': tf.placeholder(tf.float32, [None]),\n",
    "      'gestation_weeks': tf.placeholder(tf.float32, [None]),\n",
    "      'mother_married': tf.placeholder(tf.string, [None]),\n",
    "      'cigarette_use': tf.placeholder(tf.string, [None]),\n",
    "      'alcohol_use': tf.placeholder(tf.string, [None])\n",
    "    }\n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tflearn.utils.input_fn_utils.InputFnOps(\n",
    "      features,\n",
    "      None,\n",
    "      feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "\n",
    "pattern = \"csv\"\n",
    "\n",
    "def experiment_fn(output_dir):\n",
    "    wide, deep = get_wide_deep()\n",
    "    return tflearn.Experiment(\n",
    "        tflearn.DNNLinearCombinedRegressor(model_dir=output_dir,\n",
    "                                           linear_feature_columns=wide,\n",
    "                                           dnn_feature_columns=deep,\n",
    "                                           dnn_hidden_units=[64, 32]),\n",
    "        train_input_fn=read_dataset('train', pattern),\n",
    "        eval_input_fn=read_dataset('eval', pattern),\n",
    "        eval_metrics={\n",
    "            'rmse': tflearn.MetricSpec(\n",
    "                metric_fn=metrics.streaming_root_mean_squared_error\n",
    "            )\n",
    "        },\n",
    "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "            serving_input_fn,\n",
    "            default_output_alternative_key=None,\n",
    "            exports_to_keep=1\n",
    "        )],\n",
    "        min_eval_frequency=200,\n",
    "        train_steps=TRAIN_STEPS\n",
    "    )\n",
    "\n",
    "shutil.rmtree('babyweight_trained', ignore_errors=True) # start fresh each time\n",
    "learn_runner.run(experiment_fn, 'babyweight_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, the final lines of the output (above) were:\n",
    "<pre>\n",
    "INFO:tensorflow:SavedModel written to: babyweight_trained/export/Servo/1501608221496/saved_model.pb\n",
    "({'global_step': 1000, 'loss': 1.2297399, 'rmse': 1.1134831},\n",
    " ['babyweight_trained/export/Servo/1501608221496'])\n",
    "</pre>\n",
    "The Servo directory contains the final model and the final RMSE is 1.113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Monitor and experiment with training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 5359. Click <a href=\"/_proxy/44244/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5359"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('./babyweight_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorBoard, look at the learned embeddings for the race. Are they getting clustered? How about the weights for the hidden layers? What if you run this longer? What happens if you change the batchsize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TensorBoard.stop(5539)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
